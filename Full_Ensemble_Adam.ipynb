{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments using regular ensembles\n",
    "\n",
    "We start by building the model and showing the basic inference procedure and calculation of the performance on the MNIST classification and the outlier detection task. Then perform multiple runs of the model with different number of samples in the ensemble to calculate performance statistics. This experiment uses the same base learning rate as the noisy-Adam example to produce comparable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Extracting notMNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting notMNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting notMNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting notMNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Let's first setup the libraries, session and experimental data\n",
    "import experiment\n",
    "import inferences\n",
    "import edward as ed\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "s = experiment.setup()\n",
    "mnist, notmnist = experiment.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Builds the model and approximation variables used for the model\n",
    "y_, model_variables = experiment.get_model_3layer()\n",
    "approx_variables = experiment.get_pointmass_approximation_variables_3layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 5s | Loss: 220061.750\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 5s | Loss: 220065.484\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 220066.109\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 220058.016\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 220066.188\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 220072.094\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 220077.922\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 220061.062\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 220064.141\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 220071.375\n"
     ]
    }
   ],
   "source": [
    "# Performs inference with edward's MAP class and save each model state\n",
    "models = []\n",
    "num_models = 10\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.005)\n",
    "inference_dict = {model_variables[key]: val for key, val in approx_variables.iteritems()}\n",
    "\n",
    "for _ in range(num_models):\n",
    "    inference = ed.MAP(inference_dict, data={y_: model_variables['y']})\n",
    "    n_iter=1000\n",
    "    inference.initialize(n_iter=n_iter, optimizer=optimizer)\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(n_iter):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        info_dict = inference.update({model_variables['x']: batch[0],\n",
    "                                      model_variables['y']: batch[1]})\n",
    "        inference.print_progress(info_dict)\n",
    "\n",
    "    inference.finalize()\n",
    "    models.append({key: tf.Variable(val.eval()) for key, val in approx_variables.iteritems()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9489\n",
      "[ 0.08301399  0.33989117  0.05309432 ...,  0.3259176   0.05628388\n",
      "  0.02040279]\n"
     ]
    }
   ],
   "source": [
    "# Computes the accuracy of our model\n",
    "accuracy, disagreement = experiment.get_metrics_ensemble(model_variables, models, num_samples=10)\n",
    "tf.global_variables_initializer().run()\n",
    "print(accuracy.eval({model_variables['x']: mnist.test.images, model_variables['y']: mnist.test.labels}))\n",
    "print(disagreement.eval({model_variables['x']: mnist.test.images, model_variables['y']: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FP': 217, 'TN': 9783, 'FN': 4557, 'TP': 5443}\n",
      "TP/(FN+TP): 0.5443\n",
      "FP/(FP+TN): 0.0217\n"
     ]
    }
   ],
   "source": [
    "# Computes some statistics for the proposed outlier detection\n",
    "outlier_stats = experiment.get_outlier_stats(model_variables, disagreement, mnist, notmnist)\n",
    "print(outlier_stats)\n",
    "print('TP/(FN+TP): {}'.format(float(outlier_stats['TP']) / (outlier_stats['TP'] + outlier_stats['FN'])))\n",
    "print('FP/(FP+TN): {}'.format(float(outlier_stats['FP']) / (outlier_stats['FP'] + outlier_stats['TN'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The following cell performs multiple runs of this model with different number of samples within the ensemble to capture performance statistics. Results are saved in `Full_Ensemble_Adam.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 220074.953\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 220079.047\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 220066.672\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 220065.047\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 220067.953\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 220071.719\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 220073.484\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 220064.703\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 220063.734\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 220074.625\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 220061.328\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 220070.281\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220063.031\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 220075.531\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 220058.234\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 220065.062\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 220069.984\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220068.297\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220064.453\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220070.688\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220061.703\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220057.094\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220066.406\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220058.547\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220060.422\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220066.188\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220077.469\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220070.078\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220067.234\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220066.219\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220066.953\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220080.109\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 220063.266\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220075.828\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220059.312\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220081.344\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220068.234\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220071.344\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220062.781\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220073.062\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220073.328\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220077.438\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220074.172\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 220087.234\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 220064.266\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 220064.969\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 220069.250\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 220066.781\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 220075.547\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 220062.375\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 220065.188\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 220064.891\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 220064.062\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 220074.078\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 220068.469\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 220073.078\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 220066.281\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 220071.484\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 9s | Loss: 220060.797\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 220057.938\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 220067.031\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 220062.797\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 220075.156\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 220078.688\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 220065.547\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 220066.969\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 220068.766\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 220057.797\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 9s | Loss: 220061.750\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 9s | Loss: 220065.391\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 9s | Loss: 220064.969\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 9s | Loss: 220081.250\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 9s | Loss: 220071.344\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 9s | Loss: 220063.812\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 9s | Loss: 220068.672\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame(columns=('run', 'samples', 'acc', 'TP', 'FN', 'TN', 'FP'))\n",
    "\n",
    "for run in range(5):\n",
    "    models = []\n",
    "    num_models = 15\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(0.005)\n",
    "    inference_dict = {model_variables[key]: val for key, val in approx_variables.iteritems()}\n",
    "\n",
    "    for _ in range(num_models):\n",
    "        inference = ed.MAP(inference_dict, data={y_: model_variables['y']})\n",
    "        n_iter=1000\n",
    "        inference.initialize(n_iter=n_iter, optimizer=optimizer)\n",
    "\n",
    "        tf.global_variables_initializer().run()\n",
    "        for i in range(n_iter):\n",
    "            batch = mnist.train.next_batch(100)\n",
    "            info_dict = inference.update({model_variables['x']: batch[0],\n",
    "                                          model_variables['y']: batch[1]})\n",
    "            inference.print_progress(info_dict)\n",
    "\n",
    "        inference.finalize()\n",
    "        models.append({key: tf.Variable(val.eval()) for key, val in approx_variables.iteritems()})\n",
    "    \n",
    "    for num_samples in range(15):\n",
    "        accuracy, disagreement = experiment.get_metrics_ensemble(model_variables, models,\n",
    "                                                                 num_samples=num_samples + 1)\n",
    "        tf.global_variables_initializer().run()\n",
    "        acc = accuracy.eval({model_variables['x']: mnist.test.images, model_variables['y']: mnist.test.labels})\n",
    "        outlier_stats = experiment.get_outlier_stats(model_variables, disagreement, mnist, notmnist)\n",
    "        results.loc[len(results)] = [run, num_samples + 1, acc,\n",
    "                                     outlier_stats['TP'], outlier_stats['FN'],\n",
    "                                     outlier_stats['TN'], outlier_stats['FP']]\n",
    "results.to_csv('Full_Ensemble_Adam.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
