{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments using regular ensembles\n",
    "\n",
    "We start by building the model and showing the basic inference procedure and calculation of the performance on the MNIST classification and the outlier detection task. Then perform multiple runs of the model with different number of samples in the ensemble to calculate performance statistics. This experiment uses the same learning rate schedule as the SGLD example for comparable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Extracting notMNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting notMNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting notMNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting notMNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Let's first setup the libraries, session and experimental data\n",
    "import experiment\n",
    "import inferences\n",
    "import edward as ed\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "s = experiment.setup()\n",
    "mnist, notmnist = experiment.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Builds the model and approximation variables used for the model\n",
    "y_, model_variables = experiment.get_model_3layer()\n",
    "approx_variables = experiment.get_pointmass_approximation_variables_3layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 5s | Loss: 221189.375\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 5s | Loss: 221192.859\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 5s | Loss: 221169.828\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 5s | Loss: 221182.047\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221175.859\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 5s | Loss: 221172.453\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221183.641\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 5s | Loss: 221185.312\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221198.953\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221198.094\n"
     ]
    }
   ],
   "source": [
    "# Performs inference with edward's MAP class and save each model state\n",
    "models = []\n",
    "num_models = 10\n",
    "\n",
    "lr = tf.placeholder(tf.float32, shape=[])\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "inference_dict = {model_variables[key]: val for key, val in approx_variables.iteritems()}\n",
    "\n",
    "for _ in range(num_models):\n",
    "    inference = ed.MAP(inference_dict, data={y_: model_variables['y']})\n",
    "    n_iter=1000\n",
    "    inference.initialize(n_iter=n_iter, optimizer=optimizer)\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(n_iter):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        info_dict = inference.update({model_variables['x']: batch[0],\n",
    "                                      model_variables['y']: batch[1],\n",
    "                                     lr:0.005/(i+1.)})\n",
    "        inference.print_progress(info_dict)\n",
    "\n",
    "    inference.finalize()\n",
    "    models.append({key: tf.Variable(val.eval()) for key, val in approx_variables.iteritems()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8729\n",
      "[ 0.49893495  1.93824971  0.23904318 ...,  0.96060151  1.65797067\n",
      "  0.3496128 ]\n"
     ]
    }
   ],
   "source": [
    "# Computes the accuracy of our model\n",
    "accuracy, disagreement = experiment.get_metrics_ensemble(model_variables, models, num_samples=10)\n",
    "tf.global_variables_initializer().run()\n",
    "print(accuracy.eval({model_variables['x']: mnist.test.images, model_variables['y']: mnist.test.labels}))\n",
    "print(disagreement.eval({model_variables['x']: mnist.test.images, model_variables['y']: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FP': 63, 'TN': 9937, 'FN': 705, 'TP': 9295}\n",
      "TP/(FN+TP): 0.9295\n",
      "FP/(FP+TN): 0.0063\n"
     ]
    }
   ],
   "source": [
    "# Computes some statistics for the proposed outlier detection\n",
    "outlier_stats = experiment.get_outlier_stats(model_variables, disagreement, mnist, notmnist)\n",
    "print(outlier_stats)\n",
    "print('TP/(FN+TP): {}'.format(float(outlier_stats['TP']) / (outlier_stats['TP'] + outlier_stats['FN'])))\n",
    "print('FP/(FP+TN): {}'.format(float(outlier_stats['FP']) / (outlier_stats['FP'] + outlier_stats['TN'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell performs multiple runs of this model with different number of samples within the ensemble to capture performance statistics. Results are saved in `Full_Ensemble_SGLD_LR.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221189.812\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221188.016\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221169.609\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221177.719\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221203.828\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221169.047\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221203.672\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221168.969\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221193.344\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221178.297\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221196.844\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221177.078\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221176.016\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221189.188\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221193.516\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221192.016\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221169.172\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221223.391\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221188.016\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221184.953\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221175.484\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221176.672\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221185.594\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 221187.172\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221198.109\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221194.438\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221192.875\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221208.266\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221181.875\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221196.500\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221182.406\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221195.703\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221210.656\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221202.203\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221165.766\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221196.031\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221207.094\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221185.094\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221208.906\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221187.812\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221203.828\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221171.406\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221201.141\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221178.062\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221193.156\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221171.766\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221207.141\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221186.078\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221177.625\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221217.547\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 221200.234\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 221192.719\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 221189.906\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 221172.484\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 221202.125\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 221202.203\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 221191.969\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 221184.203\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 221182.219\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 9s | Loss: 221181.766\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 221178.266\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 221184.172\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 221200.281\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 221194.859\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 221174.547\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 221189.375\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 221187.953\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 221201.219\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 221185.281\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 221193.250\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 221179.609\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 221212.547\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 221193.141\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 9s | Loss: 221195.594\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 9s | Loss: 221194.766\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame(columns=('run', 'samples', 'acc', 'TP', 'FN', 'TN', 'FP'))\n",
    "\n",
    "for run in range(5):\n",
    "    models = []\n",
    "    num_models = 15\n",
    "\n",
    "    lr = tf.placeholder(tf.float32, shape=[])\n",
    "    optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "    inference_dict = {model_variables[key]: val for key, val in approx_variables.iteritems()}\n",
    "\n",
    "    for _ in range(num_models):\n",
    "        inference = ed.MAP(inference_dict, data={y_: model_variables['y']})\n",
    "        n_iter=1000\n",
    "        inference.initialize(n_iter=n_iter, optimizer=optimizer)\n",
    "\n",
    "        tf.global_variables_initializer().run()\n",
    "        for i in range(n_iter):\n",
    "            batch = mnist.train.next_batch(100)\n",
    "            info_dict = inference.update({model_variables['x']: batch[0],\n",
    "                                          model_variables['y']: batch[1],\n",
    "                                         lr:0.005/(i+1.)})\n",
    "            inference.print_progress(info_dict)\n",
    "\n",
    "        inference.finalize()\n",
    "        models.append({key: tf.Variable(val.eval()) for key, val in approx_variables.iteritems()})\n",
    "    \n",
    "    for num_samples in range(15):\n",
    "        accuracy, disagreement = experiment.get_metrics_ensemble(model_variables, models,\n",
    "                                                                 num_samples=num_samples + 1)\n",
    "        tf.global_variables_initializer().run()\n",
    "        acc = accuracy.eval({model_variables['x']: mnist.test.images, model_variables['y']: mnist.test.labels})\n",
    "        outlier_stats = experiment.get_outlier_stats(model_variables, disagreement, mnist, notmnist)\n",
    "        results.loc[len(results)] = [run, num_samples + 1, acc,\n",
    "                                     outlier_stats['TP'], outlier_stats['FN'],\n",
    "                                     outlier_stats['TN'], outlier_stats['FP']]\n",
    "results.to_csv('Full_Ensemble_SGLD_LR.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
