{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments approximating the posterior with diagonal Gaussians from noisy-Adam samples\n",
    "\n",
    "We start by building the model and showing the basic inference procedure and calculation of the performance on the MNIST classification and the outlier detection task. Then perform multiple runs of the model with different number of samples in the ensemble to calculate performance statistics. Instead of SGLD we use an adaptation of Adam which is based on ideas from SGLD. So we add noise to the parameter updates in the order adaptive learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Extracting notMNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting notMNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting notMNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting notMNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Let's first setup the libraries, session and experimental data\n",
    "import experiment\n",
    "import inferences\n",
    "import edward as ed\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "s = experiment.setup()\n",
    "mnist, notmnist = experiment.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Builds the model and approximation variables used for the model\n",
    "y_, model_variables = experiment.get_model_3layer()\n",
    "approx_variables = experiment.get_gauss_approximation_variables_3layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 10s | Acceptance Rate: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Performs inference with our custom inference class\n",
    "inference_dict = {model_variables[key]: val for key, val in approx_variables.iteritems()}\n",
    "inference = inferences.VariationalGaussNoisyAdam(inference_dict, data={y_: model_variables['y']})\n",
    "n_iter=1000\n",
    "\n",
    "inference.initialize(n_iter=n_iter, learning_rate=0.005, burn_in=0)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "for i in range(n_iter):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    info_dict = inference.update({model_variables['x']: batch[0],\n",
    "                                  model_variables['y']: batch[1]})\n",
    "    inference.print_progress(info_dict)\n",
    "\n",
    "inference.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9381\n",
      "[ 0.0472862   0.14275207  0.12083956 ...,  0.32742214  0.13082395\n",
      "  0.04032538]\n"
     ]
    }
   ],
   "source": [
    "# Computes the accuracy of our model\n",
    "accuracy, disagreement = experiment.get_metrics(model_variables, approx_variables, num_samples=10)\n",
    "print(accuracy.eval({model_variables['x']: mnist.test.images, model_variables['y']: mnist.test.labels}))\n",
    "print(disagreement.eval({model_variables['x']: mnist.test.images, model_variables['y']: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FP': 186, 'TN': 9814, 'FN': 8054, 'TP': 1946}\n",
      "TP/(FN+TP): 0.1946\n",
      "FP/(FP+TN): 0.0186\n"
     ]
    }
   ],
   "source": [
    "# Computes some statistics for the proposed outlier detection\n",
    "outlier_stats = experiment.get_outlier_stats(model_variables, disagreement, mnist, notmnist)\n",
    "print(outlier_stats)\n",
    "print('TP/(FN+TP): {}'.format(float(outlier_stats['TP']) / (outlier_stats['TP'] + outlier_stats['FN'])))\n",
    "print('FP/(FP+TN): {}'.format(float(outlier_stats['FP']) / (outlier_stats['FP'] + outlier_stats['TN'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The following cell performs multiple runs of this model with different number of samples within the ensemble to capture performance statistics. Results are saved in `SampledGauss_noisyAdam.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 10s | Acceptance Rate: 1.000\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 12s | Acceptance Rate: 1.000\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 12s | Acceptance Rate: 1.000\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 13s | Acceptance Rate: 1.000\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 13s | Acceptance Rate: 1.000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame(columns=('run', 'samples', 'acc', 'TP', 'FN', 'TN', 'FP'))\n",
    "\n",
    "for run in range(5):\n",
    "    inference_dict = {model_variables[key]: val for key, val in approx_variables.iteritems()}\n",
    "    inference = inferences.VariationalGaussNoisyAdam(inference_dict, data={y_: model_variables['y']})\n",
    "    n_iter=1000\n",
    "\n",
    "    inference.initialize(n_iter=n_iter, learning_rate=0.005, burn_in=0)\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(n_iter):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        info_dict = inference.update({model_variables['x']: batch[0],\n",
    "                                      model_variables['y']: batch[1]})\n",
    "        inference.print_progress(info_dict)\n",
    "\n",
    "    inference.finalize()\n",
    "    \n",
    "    for num_samples in range(15):\n",
    "        accuracy, disagreement = experiment.get_metrics(model_variables, approx_variables,\n",
    "                                                        num_samples=num_samples + 1)\n",
    "        acc = accuracy.eval({model_variables['x']: mnist.test.images, model_variables['y']: mnist.test.labels})\n",
    "        outlier_stats = experiment.get_outlier_stats(model_variables, disagreement, mnist, notmnist)\n",
    "        results.loc[len(results)] = [run, num_samples + 1, acc,\n",
    "                                     outlier_stats['TP'], outlier_stats['FN'],\n",
    "                                     outlier_stats['TN'], outlier_stats['FP']]\n",
    "results.to_csv('SampledGauss_noisyAdam.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
